{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi Oleh Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NeuralNet:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.nodes_n_in_hidden_layers = nodes_n_in_hidden_layers\n",
    "        self.inputs = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.outputs = [] # output dari setiap node pada satu iterasi\n",
    "        self.weights = [] # weight dari setiap edge pada satu iterasi\n",
    "        self.biases = [] # bias dari setiap node pada satu iterasi\n",
    "        self.weight_biases = []\n",
    "        self.local_gradients = [] # local gradient dari setiap node pada satu iterasi\n",
    "        self.delta_weights = [] # delta weight dari setiap edge pada satu iterasi\n",
    "        self.delta_biases = [] # delta bias dari setiap node pada satu iterasi\n",
    "        self.layer_nodes = [] # node-node pada layer-layer\n",
    "        self.v = [] # v pada setiap node\n",
    "        self.targets = []\n",
    "    \n",
    "    # Feed Forward\n",
    "    def feed_forward(self, datum_idx):\n",
    "        for i in range (1, len(self.layer_nodes)):\n",
    "            for j in range (0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                weights = []\n",
    "                weights.append(self.biases[current_node])\n",
    "                inputs = []\n",
    "                inputs.append(1)\n",
    "                for k in range(0, len(self.layer_nodes[i-1])):\n",
    "                    if (self.weights[self.layer_nodes[i-1][k]][current_node] != None):\n",
    "                            weights.append(self.weights[self.layer_nodes[i-1][k]][current_node])\n",
    "                    \n",
    "                    if (i==1):\n",
    "                        inputs.append(self.inputs[datum_idx][self.layer_nodes[i-1][k]])\n",
    "                    else:\n",
    "                        inputs.append(self.outputs[self.layer_nodes[i-1][k]])\n",
    "                v = np.dot(inputs, weights)\n",
    "                self.v[current_node] = v\n",
    "                self.outputs[current_node] = sigmoid(v)\n",
    "                \n",
    "    # Back Propagation\n",
    "    def back_propagation(self, datum_idx):\n",
    "        for i in range(len(self.layer_nodes)-1, 0, -1):\n",
    "            for j in range(len(self.layer_nodes[i])-1, -1, -1):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                if (i == len(self.layer_nodes)-1):\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * (self.targets[datum_idx] - self.outputs[current_node]))\n",
    "                else:\n",
    "                    v = self.v[current_node]\n",
    "                    sig_v = sigmoid(v)\n",
    "                    weight_delta = 1\n",
    "                    for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                        if (self.weights[current_node][self.layer_nodes[i+1][k]] != None):\n",
    "                            weight_delta = weight_delta * self.local_gradients[self.layer_nodes[i+1][k]] * self.weights[current_node][self.layer_nodes[i+1][k]]\n",
    "                    self.local_gradients[current_node] = self.local_gradients[current_node] + (sig_v * (1 - sig_v) * weight_delta)            \n",
    "                \n",
    "    # Update Weight\n",
    "    def update_weight(self):\n",
    "        for i in range(0, len(self.layer_nodes)-1):\n",
    "            for j in range(0, len(self.layer_nodes[i])):\n",
    "                current_node = self.layer_nodes[i][j]\n",
    "                for k in range(0, len(self.layer_nodes[i+1])):\n",
    "                    current_next_node = self.layer_nodes[i+1][k]\n",
    "                    self.delta_weights[current_node][current_next_node] = self.momentum * self.delta_weights[current_node][current_next_node] + self.learning_rate * self.local_gradients[current_next_node] * self.outputs[current_next_node]\n",
    "                    self.weights[current_node][current_next_node] += self.delta_weights[current_node][current_next_node]\n",
    "        for i in range(1, len(self.biases)):\n",
    "            new_bias = self.biases[i] + self.momentum  * self.delta_biases[i] + self.learning_rate * self.local_gradients[i]\n",
    "            self.delta_biases[i] = new_bias - self.biases[i]\n",
    "            self.biases[i] = new_bias\n",
    "            \n",
    "    # Fit\n",
    "    def fit(self, data, batch_size, max_iter): # data = array of arrays\n",
    "        #data[0] ke n merupakan label\n",
    "        #nodes_n_in_hidden_layers[0] merupakan jumlah input\n",
    "        self.nodes_n_in_hidden_layers.insert(0, len(data[0])-1) \n",
    "        \n",
    "        inputs = data\n",
    "        for i in range (0, len(inputs)):\n",
    "            self.targets.append(inputs[i].pop(len(inputs[i])-1))\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        n_nodes = 0\n",
    "        init_weight = 1 # Weights diinisalisasi 0\n",
    "        \n",
    "        # Inisialisasi output, bias, local gradient, v, dan delta bias di setiap node pada layer\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)):\n",
    "            l_nodes = []\n",
    "            for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                self.outputs.append(0)\n",
    "                self.v.append(0)\n",
    "                self.biases.append(0) #asumsi x bias = 1\n",
    "                self.local_gradients.append(0)\n",
    "                self.delta_biases.append(0)\n",
    "                l_nodes.append(n_nodes)\n",
    "                n_nodes += 1\n",
    "            self.layer_nodes.append(l_nodes)\n",
    "        \n",
    "        for i in range(0, n_nodes):\n",
    "            self.weights.append([])\n",
    "            self.delta_weights.append([])\n",
    "            for j in range(0, n_nodes):\n",
    "                self.weights[i].append(None)\n",
    "                self.delta_weights[i].append(None)\n",
    "            \n",
    "        current_node = 0\n",
    "        for i in range(0, len(self.nodes_n_in_hidden_layers)-1):\n",
    "            if (i < len(self.nodes_n_in_hidden_layers)-1):\n",
    "                next_layer_first_node = current_node + self.nodes_n_in_hidden_layers[i]\n",
    "                for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                    for k in range(0, self.nodes_n_in_hidden_layers[i+1]):\n",
    "                        self.weights[current_node][k+next_layer_first_node] = random.uniform(-1,1)\n",
    "                        self.delta_weights[current_node][k+next_layer_first_node] = 0\n",
    "                    current_node += 1\n",
    "        \n",
    "        n_batch = math.ceil(len(data)/batch_size)\n",
    "        \n",
    "        n_iter = 0\n",
    "        while (n_iter < max_iter):\n",
    "            \n",
    "            datum_idx = 0\n",
    "            for i in range(0, n_batch):\n",
    "                j = 0\n",
    "                while (j < batch_size):\n",
    "                    if (datum_idx < len(data)):\n",
    "                        self.feed_forward(datum_idx)\n",
    "                        self.back_propagation(datum_idx)\n",
    "                        datum_idx += 1\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        j = batch_size + 1\n",
    "                self.update_weight()\n",
    "                print(self.local_gradients)\n",
    "                # Mengembalikan local_gradient menjadi 0\n",
    "                current_node = 0\n",
    "                for i in range(0, len(self.nodes_n_in_hidden_layers)-1):\n",
    "                    if (i < len(self.nodes_n_in_hidden_layers)-1):\n",
    "                        next_layer_first_node = current_node + self.nodes_n_in_hidden_layers[i]\n",
    "                        for j in range(0, self.nodes_n_in_hidden_layers[i]):\n",
    "                            for k in range(0, self.nodes_n_in_hidden_layers[i+1]):\n",
    "                                self.local_gradients[current_node] = 0\n",
    "                            current_node += 1\n",
    "            \n",
    "            n_iter += 1\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, data_test):\n",
    "        test_outputs = []\n",
    "        feed_forward_result = []\n",
    "        self.inputs = data_test\n",
    "        \n",
    "        for i in range (0, len(data_test)):\n",
    "            self.feed_forward(i)\n",
    "            test_outputs.append(self.outputs[-1])\n",
    "        \n",
    "        return test_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Hasil Implementasi Kelompok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, -0.007543394923094519, -0.003934148800229227, 0.10949891239177356]\n",
      "[0, 0, -0.00471827049332919, -0.002400483588790515, 0.06913984759345562]\n",
      "[0, 0, -0.011893103288198597, -0.006010097429987287, 0.17733708027047124]\n",
      "[0, 0, -0.008854404992748845, -0.0042971035100990256, 0.13479611289539178]\n",
      "[0, 0, -0.015358011985116384, -0.007283285106416501, 0.24061356898901226]\n",
      "[0, 0, -0.012021005412282921, -0.005348003201821902, 0.19475522666282277]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4679143495984448, 0.4906959371444368]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet([2], 0.1, 0.01) \n",
    "nn.fit([[0.1,0.9,0.9],[0.1,0.88,0.3]], 1, 3)\n",
    "data_test = [[0.9,0.1],[0.1,0.9]]\n",
    "nn.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementasi dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "class KerasNeuralNet:\n",
    "    \n",
    "    # Construction\n",
    "    def __init__(self, nodes_n_in_hidden_layers, learning_rate, momentum):\n",
    "        nodes_n_in_hidden_layers.append(1) # satu node buat output\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[0], input_dim=4, activation=\"sigmoid\"))\n",
    "        for i in range(1, len(nodes_n_in_hidden_layers)):\n",
    "            self.model.add(Dense(output_dim=nodes_n_in_hidden_layers[i], input_dim=nodes_n_in_hidden_layers[i-1], activation='sigmoid'))\n",
    "        sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "        \n",
    "    def fit(self, X, Y, batch_size):\n",
    "        self.model.fit(X, Y, batch_size=batch_size)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=4, activation=\"sigmoid\", units=2)`\n",
      "  del sys.path[0]\n",
      "/home/suzaneringoringo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=2, activation=\"sigmoid\", units=1)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   0.5  0.   0. ]\n",
      " [ 1.   0.5  0.   1. ]\n",
      " [ 0.   0.5  0.   0. ]\n",
      " [ 0.5  1.   0.   0. ]\n",
      " [ 0.5  0.   1.   0. ]\n",
      " [ 0.5  0.   1.   1. ]\n",
      " [ 0.   0.   1.   1. ]\n",
      " [ 1.   1.   0.   0. ]\n",
      " [ 1.   0.   1.   0. ]\n",
      " [ 0.5  1.   1.   0. ]\n",
      " [ 1.   1.   1.   1. ]\n",
      " [ 0.   1.   0.   1. ]\n",
      " [ 0.   0.5  1.   0. ]\n",
      " [ 0.5  1.   0.   1. ]] [0 0 1 1 1 0 1 0 1 1 1 1 1 0] 1\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 1s 82ms/step - loss: 0.8824 - acc: 0.3571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.18783519],\n",
       "       [ 0.22113007],\n",
       "       [ 0.25110272],\n",
       "       [ 0.21877776],\n",
       "       [ 0.3226575 ],\n",
       "       [ 0.36289889],\n",
       "       [ 0.39556172],\n",
       "       [ 0.18942542],\n",
       "       [ 0.28445289],\n",
       "       [ 0.32363689],\n",
       "       [ 0.32989258],\n",
       "       [ 0.29406101],\n",
       "       [ 0.35958937],\n",
       "       [ 0.25698826]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data\n",
    "dataframe = pd.read_csv('tennis.csv')\n",
    "\n",
    "# Transform outlook, temperature, humidity, and windy to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded = dataframe.apply(le.fit_transform)\n",
    "dataset = encoded.values\n",
    "\n",
    "# X and Y values\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# Rescale min and max for X\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "knn = KerasNeuralNet([2], 0.25, 0.0001)\n",
    "\n",
    "# Train model\n",
    "knn.fit(rescaledX, Y, 1)\n",
    "\n",
    "# Predict\n",
    "knn.predict(rescaledX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
